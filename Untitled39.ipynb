{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb9b81f3-c82c-4771-a929-4b1cb54b88cc",
   "metadata": {},
   "source": [
    "#### Q1. What is boosting in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6690c83-2c02-4c2c-ab33-c94bb1d8947d",
   "metadata": {},
   "source": [
    "# Boosting in Machine Learning\n",
    "\n",
    "Boosting is a powerful ensemble technique in machine learning used to improve the accuracy of models by combining the outputs of several weak learners to create a strong learner. The key idea behind boosting is to sequentially apply weak learning algorithms to repeatedly modified versions of the data, focusing on the mistakes made by previous models.\n",
    "\n",
    "In this notebook, we will use the `AdaBoost` algorithm to demonstrate how boosting works. We'll apply it to the Iris dataset, train the model, and evaluate its performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0824f308-b951-422d-a244-78b7f2fbba2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\dell\\anaconda3\\lib\\site-packages (1.2.2)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from scikit-learn) (1.11.4)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ccd691b-7b06-4717-9e4f-80898241cf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ec88a66-cf75-47c7-b4a3-ddb07e9d3c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f3752bc-d6e1-43f0-b2c2-3ff9a981bdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a2c3e614-3821-4144-80ad-b1a2fe6f96d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dell\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1),\n",
       "                   random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">AdaBoostClassifier</label><div class=\"sk-toggleable__content\"><pre>AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1),\n",
       "                   random_state=42)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">base_estimator: DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(max_depth=1)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(max_depth=1)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1),\n",
       "                   random_state=42)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the base estimator (weak learner)\n",
    "base_estimator = DecisionTreeClassifier(max_depth=1)\n",
    "\n",
    "# Initialize the AdaBoost classifier\n",
    "ada_boost = AdaBoostClassifier(base_estimator=base_estimator, n_estimators=50, learning_rate=1.0, random_state=42)\n",
    "\n",
    "# Train the AdaBoost model\n",
    "ada_boost.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c2ef2cca-011e-4226-93ba-ef4eef5b8b3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the test set\n",
    "y_pred = ada_boost.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a8404c73-64f6-40e0-acdc-6ea631935103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Importance\n",
      "petal length (cm)         0.5\n",
      "petal width (cm)          0.5\n",
      "sepal length (cm)         0.0\n",
      "sepal width (cm)          0.0\n"
     ]
    }
   ],
   "source": [
    "# Display feature importances\n",
    "importances = ada_boost.feature_importances_\n",
    "features = iris.feature_names\n",
    "feature_importances = pd.DataFrame(importances, index=features, columns=['Importance']).sort_values(by='Importance', ascending=False)\n",
    "print(feature_importances)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278f5cbb-29a1-468d-aef8-dfeac58bfb82",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In this notebook, we demonstrated how boosting works by using the `AdaBoost` algorithm on the Iris dataset. We achieved a good accuracy score, indicating the effectiveness of boosting in improving model performance. We also examined the feature importances to understand which features contributed most to the model's predictions.\n",
    "\n",
    "Possible improvements to this pipeline include experimenting with different base estimators, tuning hyperparameters, and using other boosting algorithms like Gradient Boosting or XGBoost.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a69ee8-999c-41cf-be1d-6d22b4dd0cf6",
   "metadata": {},
   "source": [
    "#### Q2. What are the advantages and limitations of using boosting techniques?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2ae35c-b9b7-4ad4-94c5-4ae4ed47830e",
   "metadata": {},
   "source": [
    "# Advantages and Limitations of Boosting Techniques\n",
    "\n",
    "Boosting is a powerful ensemble technique in machine learning that aims to create a strong learner by combining the outputs of several weak learners. It has several advantages and some limitations, which are important to understand for effectively applying boosting algorithms in practice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488c9410-52cf-42bb-a002-9485c93df3a1",
   "metadata": {},
   "source": [
    "## Advantages of Boosting\n",
    "\n",
    "1. **Improved Accuracy**:\n",
    "   Boosting often significantly improves the accuracy of machine learning models by combining multiple weak learners to form a strong learner.\n",
    "\n",
    "2. **Handling of Bias**:\n",
    "   Boosting helps in reducing bias by focusing on the errors made by previous models, thus improving overall model performance.\n",
    "\n",
    "3. **Versatility**:\n",
    "   Boosting can be applied to a wide range of models, from decision trees to linear models, making it a versatile technique.\n",
    "\n",
    "4. **Robustness to Overfitting**:\n",
    "   Some boosting algorithms, like AdaBoost, are robust to overfitting, especially when used with simple models like decision stumps.\n",
    "\n",
    "5. **Feature Importance**:\n",
    "   Boosting algorithms often provide insights into feature importance, which can be useful for understanding the underlying data and for feature selection.\n",
    "\n",
    "6. **Flexibility**:\n",
    "   Boosting can be used for both classification and regression tasks, offering a flexible approach to various types of machine learning problems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78274ad6-f38e-451b-b511-4c759b38432f",
   "metadata": {},
   "source": [
    "## Limitations of Boosting\n",
    "\n",
    "1. **Sensitivity to Noise**:\n",
    "   Boosting algorithms can be sensitive to noisy data and outliers since they focus on correcting errors, which may lead to overfitting on noisy datasets.\n",
    "\n",
    "2. **Computationally Intensive**:\n",
    "   Boosting can be computationally expensive and time-consuming, especially with large datasets and complex base learners, due to its iterative nature.\n",
    "\n",
    "3. **Complexity**:\n",
    "   The boosted model can become complex and difficult to interpret, particularly with a large number of weak learners.\n",
    "\n",
    "4. **Overfitting with Complex Models**:\n",
    "   While boosting is generally robust to overfitting, it can still overfit if the base learners are too complex.\n",
    "\n",
    "5. **Parameter Tuning**:\n",
    "   Boosting algorithms often require careful tuning of hyperparameters, such as the learning rate and the number of weak learners, which can be challenging and time-consuming.\n",
    "\n",
    "6. **Implementation Challenges**:\n",
    "   Implementing boosting algorithms correctly can be more complex than other ensemble techniques, requiring a good understanding of the algorithm's mechanics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbd8dac-d2df-4180-ab23-944ce6917134",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Boosting is a powerful and versatile ensemble technique that can significantly improve the performance of machine learning models. However, it is essential to be aware of its limitations, such as sensitivity to noise and computational complexity. Understanding both the advantages and limitations of boosting allows practitioners to effectively apply this technique to various machine learning problems and achieve better results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba62037a-45d6-4b30-bcf8-5614632cb553",
   "metadata": {},
   "source": [
    "### Q3. Explain how boosting works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435cb904-75e3-4491-9f14-a76e251fe7b6",
   "metadata": {},
   "source": [
    "Boosting is an ensemble learning technique that aims to create a strong learner by sequentially combining multiple weak learners. The core idea is to focus on the mistakes of previous models and correct them in subsequent models. This process continues until a specified number of weak learners are combined or the performance no longer improves. Here’s a step-by-step explanation of how boosting works:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef99484-0141-4d8a-b22d-7b75ef2df385",
   "metadata": {},
   "source": [
    "## Detailed Explanation\n",
    "\n",
    "1. **Initialize Weights**:\n",
    "   - Start by assigning equal weights to all training samples. These weights represent the importance of each sample in the dataset.\n",
    "\n",
    "2. **Train Weak Learner**:\n",
    "   - Train a weak learner (e.g., a decision stump) on the weighted training data. A weak learner is a model that performs slightly better than random guessing.\n",
    "\n",
    "3. **Evaluate Weak Learner**:\n",
    "   - Evaluate the performance of the weak learner on the training data. Calculate the error rate, which is the weighted sum of the misclassified samples.\n",
    "\n",
    "4. **Update Weights**:\n",
    "   - Increase the weights of the misclassified samples and decrease the weights of the correctly classified samples. This way, the next weak learner focuses more on the difficult cases that were misclassified by the previous learner.\n",
    "\n",
    "5. **Combine Weak Learners**:\n",
    "   - Assign a weight to the weak learner based on its performance (learners with lower error rates get higher weights). Combine the weak learners to form a strong learner. This combination can be a weighted majority vote (for classification) or a weighted sum (for regression).\n",
    "\n",
    "6. **Repeat**:\n",
    "   - Repeat steps 2-5 for a specified number of iterations or until the error rate reaches an acceptable level. Each iteration improves the model by focusing more on the hard-to-classify samples.\n",
    "\n",
    "7. **Final Model**:\n",
    "   - The final model is a weighted combination of all the weak learners. This ensemble model usually has much better performance than any of the individual weak learners.\n",
    "\n",
    "### Algorithm\n",
    "\n",
    "Let's break down the algorithm with a simple example using `AdaBoost`, one of the most popular boosting algorithms:\n",
    "\n",
    "### Pseudo-Code for AdaBoost\n",
    "\n",
    "1. **Initialize weights** \\( w_i = \\frac{1}{N} \\) for all \\( i = 1, ..., N \\)\n",
    "2. For \\( t = 1 \\) to \\( T \\) (number of iterations):\n",
    "   1. Train a weak learner \\( h_t \\) on the training data with weights \\( w \\)\n",
    "   2. Compute the error rate \\( \\epsilon_t \\) of \\( h_t \\)\n",
    "   3. Compute the learner's weight \\( \\alpha_t = \\ln \\left(\\frac{1 - \\epsilon_t}{\\epsilon_t}\\right) \\)\n",
    "   4. Update weights \\( w_i \\) for each training sample:\n",
    "      - Increase weights for misclassified samples\n",
    "      - Decrease weights for correctly classified samples\n",
    "      - Normalize the weights\n",
    "3. **Final model**: Combine the weak learners \\( H(x) = \\text{sign} \\left(\\sum_{t=1}^{T} \\alpha_t h_t(x)\\right) \\)\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Boosting builds a robust model by iteratively focusing on the mistakes made by previous models. This method reduces bias and variance, leading to improved performance on various machine learning tasks. However, it requires careful tuning of parameters and can be computationally intensive.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41aca37-204b-437d-b451-0cd00530e35d",
   "metadata": {},
   "source": [
    "### Q4. What are the different types of boosting algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6bb7d4-a480-4e98-ab58-ae8054d6b447",
   "metadata": {},
   "source": [
    "# Different Types of Boosting Algorithms\n",
    "\n",
    "Boosting algorithms are powerful techniques that improve the performance of weak learners by combining them to form a strong learner. There are several types of boosting algorithms, each with its own unique approach to improving model performance. In this notebook, we'll discuss the most commonly used boosting algorithms: AdaBoost, Gradient Boosting, XGBoost, LightGBM, and CatBoost.\n",
    "\n",
    "## 1. AdaBoost (Adaptive Boosting)\n",
    "\n",
    "### Overview:\n",
    "AdaBoost, short for Adaptive Boosting, was one of the first boosting algorithms developed. It adjusts the weights of incorrectly classified instances so that subsequent weak learners focus more on difficult cases.\n",
    "\n",
    "### Key Characteristics:\n",
    "- Uses decision stumps (one-level decision trees) as weak learners.\n",
    "- Iteratively adjusts the weights of training instances based on the classification errors.\n",
    "- Combines the weak learners using a weighted majority vote.\n",
    "\n",
    "### Algorithm:\n",
    "1. Initialize weights for all instances.\n",
    "2. For each iteration:\n",
    "   - Train a weak learner.\n",
    "   - Calculate the error rate.\n",
    "   - Adjust the weights of misclassified instances.\n",
    "   - Compute the weight of the weak learner.\n",
    "3. Combine the weak learners to form a strong classifier.\n",
    "\n",
    "### Applications:\n",
    "- Face detection\n",
    "- Text classification\n",
    "\n",
    "## 2. Gradient Boosting\n",
    "\n",
    "### Overview:\n",
    "Gradient Boosting builds an ensemble of weak learners, typically decision trees, by sequentially adding models that correct the errors of the combined ensemble using gradient descent.\n",
    "\n",
    "### Key Characteristics:\n",
    "- Uses decision trees as base learners.\n",
    "- Optimizes a loss function by adding models that minimize the residual errors.\n",
    "- More flexible and can optimize different types of loss functions.\n",
    "\n",
    "### Algorithm:\n",
    "1. Initialize the model with a constant value.\n",
    "2. For each iteration:\n",
    "   - Compute the negative gradient (residual errors).\n",
    "   - Train a weak learner on the residuals.\n",
    "   - Update the model by adding the new weak learner.\n",
    "\n",
    "### Applications:\n",
    "- Regression and classification tasks\n",
    "- Web search ranking\n",
    "- Predictive maintenance\n",
    "\n",
    "## 3. XGBoost (Extreme Gradient Boosting)\n",
    "\n",
    "### Overview:\n",
    "XGBoost is an optimized version of Gradient Boosting that is designed for speed and performance. It includes regularization to prevent overfitting and parallel processing to improve computation time.\n",
    "\n",
    "### Key Characteristics:\n",
    "- Incorporates regularization (L1 and L2) to control model complexity.\n",
    "- Supports parallel and distributed computing.\n",
    "- Efficient handling of missing values.\n",
    "\n",
    "### Algorithm:\n",
    "1. Similar to Gradient Boosting, but with additional regularization terms.\n",
    "2. Supports various optimizations, including tree pruning and parallel processing.\n",
    "\n",
    "### Applications:\n",
    "- Kaggle competitions\n",
    "- Time-series forecasting\n",
    "- Classification and regression tasks\n",
    "\n",
    "## 4. LightGBM (Light Gradient Boosting Machine)\n",
    "\n",
    "### Overview:\n",
    "LightGBM is a gradient boosting framework that uses tree-based learning algorithms. It is designed to be efficient and scalable, especially for large datasets.\n",
    "\n",
    "### Key Characteristics:\n",
    "- Uses a leaf-wise tree growth strategy, which tends to converge faster than level-wise methods.\n",
    "- Efficient in memory usage and computation speed.\n",
    "- Capable of handling large datasets with millions of instances.\n",
    "\n",
    "### Algorithm:\n",
    "1. Similar to Gradient Boosting but uses a leaf-wise growth strategy.\n",
    "2. Incorporates techniques like histogram-based decision trees and gradient-based one-side sampling.\n",
    "\n",
    "### Applications:\n",
    "- Large-scale machine learning tasks\n",
    "- High-dimensional data\n",
    "- Online advertising and recommendation systems\n",
    "\n",
    "## 5. CatBoost (Categorical Boosting)\n",
    "\n",
    "### Overview:\n",
    "CatBoost is a gradient boosting algorithm that is particularly powerful for datasets with categorical features. It handles categorical variables natively without requiring extensive preprocessing.\n",
    "\n",
    "### Key Characteristics:\n",
    "- Efficiently handles categorical features by converting them into numerical representations.\n",
    "- Incorporates ordered boosting to reduce overfitting.\n",
    "- Robust to overfitting and supports GPU acceleration.\n",
    "\n",
    "### Algorithm:\n",
    "1. Similar to Gradient Boosting, with specialized handling for categorical features.\n",
    "2. Uses ordered boosting and other techniques to improve generalization.\n",
    "\n",
    "### Applications:\n",
    "- Datasets with many categorical features\n",
    "- E-commerce and user behavior analysis\n",
    "- Fraud detection\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Boosting algorithms are powerful tools for improving the performance of machine learning models by combining multiple weak learners into a strong learner. Each boosting algorithm has its own strengths and is suited to different types of problems and datasets. Understanding the various boosting algorithms allows practitioners to choose the most appropriate one for their specific task and achieve better results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9960ef-cd86-46fd-a604-6105b342436b",
   "metadata": {},
   "source": [
    "## Q5. What are some common parameters in boosting algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f39ad6-4f27-4adf-b65c-d05666ec85dd",
   "metadata": {},
   "source": [
    "# Common Parameters in Boosting Algorithms\n",
    "\n",
    "Boosting algorithms are powerful tools that require careful tuning of their parameters to achieve optimal performance. In this notebook, we will discuss some of the common parameters found in popular boosting algorithms such as AdaBoost, Gradient Boosting, XGBoost, LightGBM, and CatBoost.\n",
    "\n",
    "## 1. AdaBoost Parameters\n",
    "\n",
    "1. **n_estimators**:\n",
    "   - Description: The maximum number of weak learners (estimators) to train.\n",
    "   - Default: 50\n",
    "   - Impact: Increasing this value can improve performance but may also increase the risk of overfitting.\n",
    "\n",
    "2. **learning_rate**:\n",
    "   - Description: The contribution of each weak learner to the final model.\n",
    "   - Default: 1.0\n",
    "   - Impact: Lower values reduce the impact of each weak learner, requiring more estimators for a strong model.\n",
    "\n",
    "3. **base_estimator**:\n",
    "   - Description: The base learning algorithm to be used.\n",
    "   - Default: DecisionTreeClassifier with max depth 1 (decision stump)\n",
    "   - Impact: Changing the base estimator can significantly affect model performance and complexity.\n",
    "\n",
    "## 2. Gradient Boosting Parameters\n",
    "\n",
    "1. **n_estimators**:\n",
    "   - Description: The number of boosting stages to be run.\n",
    "   - Default: 100\n",
    "   - Impact: More estimators can lead to better performance but also increase training time and risk of overfitting.\n",
    "\n",
    "2. **learning_rate**:\n",
    "   - Description: Shrinks the contribution of each tree by this value.\n",
    "   - Default: 0.1\n",
    "   - Impact: A lower learning rate requires more estimators but can lead to better generalization.\n",
    "\n",
    "3. **max_depth**:\n",
    "   - Description: The maximum depth of the individual regression estimators.\n",
    "   - Default: 3\n",
    "   - Impact: Limits the complexity of each tree. Deeper trees can capture more complex patterns but may overfit.\n",
    "\n",
    "4. **subsample**:\n",
    "   - Description: The fraction of samples to be used for fitting individual base learners.\n",
    "   - Default: 1.0 (use all samples)\n",
    "   - Impact: Using a subset of samples (e.g., 0.8) can reduce overfitting and improve generalization.\n",
    "\n",
    "5. **min_samples_split**:\n",
    "   - Description: The minimum number of samples required to split an internal node.\n",
    "   - Default: 2\n",
    "   - Impact: Higher values prevent the model from learning overly specific patterns (overfitting).\n",
    "\n",
    "6. **min_samples_leaf**:\n",
    "   - Description: The minimum number of samples required to be at a leaf node.\n",
    "   - Default: 1\n",
    "   - Impact: Higher values help prevent overfitting by ensuring each leaf has enough samples.\n",
    "\n",
    "## 3. XGBoost Parameters\n",
    "\n",
    "1. **n_estimators**:\n",
    "   - Description: Number of boosting rounds.\n",
    "   - Default: 100\n",
    "   - Impact: Similar to other boosting algorithms, more rounds can improve performance but increase overfitting risk.\n",
    "\n",
    "2. **learning_rate (eta)**:\n",
    "   - Description: Step size shrinkage used to prevent overfitting.\n",
    "   - Default: 0.3\n",
    "   - Impact: Smaller values improve generalization but require more boosting rounds.\n",
    "\n",
    "3. **max_depth**:\n",
    "   - Description: Maximum depth of a tree.\n",
    "   - Default: 6\n",
    "   - Impact: Controls tree complexity. Deeper trees capture more patterns but risk overfitting.\n",
    "\n",
    "4. **subsample**:\n",
    "   - Description: Fraction of samples used for growing trees.\n",
    "   - Default: 1.0\n",
    "   - Impact: Lower values prevent overfitting and improve generalization.\n",
    "\n",
    "5. **colsample_bytree**:\n",
    "   - Description: Fraction of features to be randomly sampled for each tree.\n",
    "   - Default: 1.0\n",
    "   - Impact: Reducing this value can improve generalization and reduce overfitting.\n",
    "\n",
    "6. **gamma**:\n",
    "   - Description: Minimum loss reduction required to make a further partition on a leaf node.\n",
    "   - Default: 0\n",
    "   - Impact: Higher values make the algorithm more conservative, reducing the risk of overfitting.\n",
    "\n",
    "## 4. LightGBM Parameters\n",
    "\n",
    "1. **n_estimators**:\n",
    "   - Description: Number of boosting rounds.\n",
    "   - Default: 100\n",
    "   - Impact: Similar to other boosting algorithms, more rounds can improve performance but increase overfitting risk.\n",
    "\n",
    "2. **learning_rate**:\n",
    "   - Description: Controls the step size shrinkage to prevent overfitting.\n",
    "   - Default: 0.1\n",
    "   - Impact: Smaller values improve generalization but require more boosting rounds.\n",
    "\n",
    "3. **num_leaves**:\n",
    "   - Description: Maximum number of leaves per tree.\n",
    "   - Default: 31\n",
    "   - Impact: Increasing the number of leaves can improve accuracy but also risk overfitting.\n",
    "\n",
    "4. **max_depth**:\n",
    "   - Description: Maximum depth of a tree.\n",
    "   - Default: -1 (no limit)\n",
    "   - Impact: Limits the tree depth to prevent overfitting.\n",
    "\n",
    "5. **min_data_in_leaf**:\n",
    "   - Description: Minimum number of samples in a leaf.\n",
    "   - Default: 20\n",
    "   - Impact: Prevents overfitting by ensuring each leaf has enough samples.\n",
    "\n",
    "6. **feature_fraction**:\n",
    "   - Description: Fraction of features to be used for each boosting round.\n",
    "   - Default: 1.0\n",
    "   - Impact: Reducing this value can improve generalization and reduce overfitting.\n",
    "\n",
    "## 5. CatBoost Parameters\n",
    "\n",
    "1. **iterations**:\n",
    "   - Description: The maximum number of boosting iterations.\n",
    "   - Default: 1000\n",
    "   - Impact: More iterations can improve performance but also increase overfitting risk.\n",
    "\n",
    "2. **learning_rate**:\n",
    "   - Description: Step size shrinkage to prevent overfitting.\n",
    "   - Default: 0.03\n",
    "   - Impact: Smaller values improve generalization but require more iterations.\n",
    "\n",
    "3. **depth**:\n",
    "   - Description: Depth of the tree.\n",
    "   - Default: 6\n",
    "   - Impact: Deeper trees can capture more patterns but risk overfitting.\n",
    "\n",
    "4. **l2_leaf_reg**:\n",
    "   - Description: L2 regularization term on weights.\n",
    "   - Default: 3\n",
    "   - Impact: Higher values prevent overfitting by shrinking leaf weights.\n",
    "\n",
    "5. **bagging_temperature**:\n",
    "   - Description: Controls the intensity of Bayesian bagging.\n",
    "   - Default: 1\n",
    "   - Impact: Higher values increase the randomness and can improve generalization.\n",
    "\n",
    "6. **border_count**:\n",
    "   - Description: Number of splits for numerical features.\n",
    "   - Default: 254\n",
    "   - Impact: More splits can capture more detailed patterns but may overfit.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Boosting algorithms have several parameters that need to be carefully tuned to achieve optimal performance. Understanding these parameters and their impact on the model can help practitioners build more accurate and robust models. This overview provides a starting point for tuning boosting algorithms like AdaBoost, Gradient Boosting, XGBoost, LightGBM, and CatBoost.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793858e2-27f6-4a36-bc41-93e6a65c28b6",
   "metadata": {},
   "source": [
    "## Q6. How do boosting algorithms combine weak learners to create a strong learner?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c6d3de-f4f3-4625-b6f2-db67b5d61465",
   "metadata": {},
   "source": [
    "# How Boosting Algorithms Combine Weak Learners to Create a Strong Learner\n",
    "\n",
    "Boosting algorithms are ensemble methods that combine the outputs of several weak learners to form a strong learner. The key idea behind boosting is to sequentially apply weak learning algorithms to repeatedly modified versions of the data, thus focusing on the mistakes of the previous learners. Here’s a detailed explanation of how this process works.\n",
    "\n",
    "## General Workflow of Boosting\n",
    "\n",
    "1. **Initialization**:\n",
    "   - Initialize the model by assigning equal weights to all training instances.\n",
    "   \n",
    "2. **Training Weak Learners**:\n",
    "   - For a specified number of iterations (or until a stopping criterion is met), perform the following steps:\n",
    "     1. **Fit a Weak Learner**: Train a weak learner on the weighted training data.\n",
    "     2. **Evaluate Performance**: Assess the performance of the weak learner, typically using a loss function.\n",
    "     3. **Update Weights**: Increase the weights of the misclassified instances so that the next weak learner focuses more on these hard-to-classify instances.\n",
    "     4. **Combine Learners**: Add the weak learner to the ensemble with a weight that reflects its accuracy.\n",
    "\n",
    "3. **Prediction**:\n",
    "   - Combine the predictions of all weak learners, typically through a weighted majority vote (for classification) or a weighted sum (for regression).\n",
    "\n",
    "## Specific Boosting Algorithms\n",
    "\n",
    "### 1. AdaBoost (Adaptive Boosting)\n",
    "\n",
    "- **Initialization**: All instances start with equal weights.\n",
    "- **Training**:\n",
    "  1. Train a weak learner on the data.\n",
    "  2. Calculate the weighted error rate of the weak learner.\n",
    "  3. Compute the learner’s weight: \\( \\alpha_t = \\log((1 - error_t) / error_t) \\).\n",
    "  4. Update instance weights: Increase weights for misclassified instances.\n",
    "- **Combination**: The final model is a weighted majority vote of all weak learners.\n",
    "\n",
    "### 2. Gradient Boosting\n",
    "\n",
    "- **Initialization**: Start with an initial model, typically the mean of the target values for regression.\n",
    "- **Training**:\n",
    "  1. Compute the residuals (errors) of the current model.\n",
    "  2. Train a weak learner to predict the residuals.\n",
    "  3. Update the model: Add the weak learner’s predictions to the current model.\n",
    "- **Combination**: The final model is a sum of all weak learners, each scaled by a learning rate.\n",
    "\n",
    "### 3. XGBoost (Extreme Gradient Boosting)\n",
    "\n",
    "- **Initialization**: Similar to Gradient Boosting.\n",
    "- **Training**:\n",
    "  1. Calculate the gradients and second-order gradients (Hessians) of the loss function.\n",
    "  2. Train a weak learner on the gradients and Hessians.\n",
    "  3. Update the model: Adjust predictions based on the weak learner’s output.\n",
    "- **Combination**: Incorporates regularization terms to prevent overfitting. The final model is a sum of all weak learners.\n",
    "\n",
    "### 4. LightGBM (Light Gradient Boosting Machine)\n",
    "\n",
    "- **Initialization**: Similar to Gradient Boosting.\n",
    "- **Training**:\n",
    "  1. Use histogram-based methods for faster training.\n",
    "  2. Train on the residuals using a leaf-wise tree growth strategy.\n",
    "- **Combination**: The final model is a sum of weak learners with efficient training and lower memory usage.\n",
    "\n",
    "### 5. CatBoost (Categorical Boosting)\n",
    "\n",
    "- **Initialization**: Similar to Gradient Boosting.\n",
    "- **Training**:\n",
    "  1. Handle categorical features directly using ordered boosting.\n",
    "  2. Train on residuals with an emphasis on avoiding overfitting.\n",
    "- **Combination**: Uses ordered boosting and efficient handling of categorical data. The final model is a sum of weak learners.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Boosting algorithms work by sequentially applying weak learners to modified versions of the data, where each learner focuses more on the instances misclassified by its predecessors. This process reduces bias and variance, creating a strong learner from a series of weak learners. Understanding how these algorithms combine weak learners helps in tuning and applying boosting methods effectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11191d9-a661-441c-9f55-b8af2e58bfe5",
   "metadata": {},
   "source": [
    "## Q7. Explain the concept of AdaBoost algorithm and its working."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7baae2-5b1e-48af-9eca-03f822350dc8",
   "metadata": {},
   "source": [
    "# AdaBoost Algorithm: Adaptive Boosting\n",
    "\n",
    "AdaBoost, short for Adaptive Boosting, is one of the earliest and most popular boosting algorithms. It is an ensemble learning method that combines the predictions of multiple weak learners (often simple decision trees) to create a strong learner. The key idea behind AdaBoost is to iteratively train weak learners on modified versions of the data, where the emphasis is placed on instances that were misclassified by the previous learners. Here's how the AdaBoost algorithm works:\n",
    "\n",
    "## Algorithm Steps:\n",
    "\n",
    "1. **Initialization**:\n",
    "   - Initialize the weights of all training instances to be equal.\n",
    "\n",
    "2. **For each iteration \\(t\\)**:\n",
    "   - Train a weak learner \\(h_t\\) on the training data. This weak learner typically consists of a simple decision stump (a one-level decision tree).\n",
    "   - Calculate the weighted error rate \\(e_t\\) of the weak learner on the training data. This error rate is the sum of the weights of the misclassified instances.\n",
    "   - Compute the weight \\( \\alpha_t \\) of the weak learner based on its error rate:\n",
    "     \\[ \\alpha_t = \\log \\left( \\frac{1 - e_t}{e_t} \\right) \\]\n",
    "   - Update the weights of the training instances:\n",
    "     - Increase the weights of the misclassified instances, so they have a higher chance of being correctly classified in the next iteration.\n",
    "     - Decrease the weights of correctly classified instances.\n",
    "   - Normalize the weights so that they sum to one.\n",
    "   - Combine the weak learner \\(h_t\\) with its weight \\( \\alpha_t \\) to form the strong learner:\n",
    "     \\[ H(x) = \\sum_{t=1}^{T} \\alpha_t h_t(x) \\]\n",
    "   - Repeat until a predefined number of iterations is reached or until a stopping criterion is met.\n",
    "\n",
    "3. **Prediction**:\n",
    "   - To make predictions on new data, AdaBoost combines the predictions of all weak learners using a weighted majority vote:\n",
    "     \\[ \\hat{y} = \\text{sign}(H(x)) \\]\n",
    "\n",
    "## Key Concepts:\n",
    "\n",
    "- **Weighted Training**: AdaBoost assigns weights to training instances, where the weights are increased for misclassified instances and decreased for correctly classified instances. This way, subsequent weak learners focus more on the instances that were difficult to classify by the previous learners.\n",
    "\n",
    "- **Weighted Majority Vote**: The final model in AdaBoost is a weighted combination of all weak learners, where each weak learner's contribution is proportional to its accuracy. During prediction, the model combines the predictions of all weak learners using a weighted majority vote to make the final prediction.\n",
    "\n",
    "- **Adaptive Learning**: AdaBoost is adaptive in the sense that it adjusts the weights of training instances based on the performance of the previous weak learners. This adaptiveness allows AdaBoost to iteratively improve its performance and focus on difficult instances.\n",
    "\n",
    "## Applications:\n",
    "\n",
    "- AdaBoost is commonly used in binary classification problems.\n",
    "- It is often applied to tasks such as face detection, text classification, and medical diagnosis.\n",
    "\n",
    "## Advantages:\n",
    "\n",
    "- AdaBoost is robust to overfitting and can generalize well to unseen data.\n",
    "- It can achieve high accuracy with relatively simple weak learners.\n",
    "- It is less prone to the curse of dimensionality compared to other algorithms.\n",
    "\n",
    "## Limitations:\n",
    "\n",
    "- AdaBoost can be sensitive to noisy data and outliers.\n",
    "- It may require careful tuning of parameters such as the number of weak learners and learning rate.\n",
    "- Training AdaBoost can be computationally expensive, especially for large datasets.\n",
    "\n",
    "AdaBoost is a powerful algorithm that has been widely used in both academic research and practical applications due to its effectiveness and simplicity. By iteratively combining the predictions of weak learners, AdaBoost creates a strong ensemble model that often outperforms individual classifiers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1eca7e-ea7c-4d45-b3a8-1fc267e4dcd8",
   "metadata": {},
   "source": [
    "## Q8. What is the loss function used in AdaBoost algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71453767-a2e4-4d2e-9713-98bf7fe73e5f",
   "metadata": {},
   "source": [
    "# Loss Function Used in AdaBoost Algorithm\n",
    "\n",
    "In the AdaBoost algorithm, the loss function used is the exponential loss function. \n",
    "\n",
    "The exponential loss function is defined as:\n",
    "\n",
    "\\[ L(y, f(x)) = e^{-yf(x)} \\]\n",
    "\n",
    "Where:\n",
    "- \\( y \\) is the true label of the instance, which is either -1 or 1.\n",
    "- \\( f(x) \\) is the prediction of the model for the instance \\( x \\).\n",
    "\n",
    "This loss function penalizes misclassifications exponentially. When the prediction \\( f(x) \\) matches the true label \\( y \\), the loss is small. However, when the prediction is incorrect, the loss increases exponentially.\n",
    "\n",
    "During each iteration of AdaBoost, the weak learner is trained to minimize this exponential loss function, aiming to improve its classification accuracy on the weighted training data. The weights of the training instances are adjusted based on the misclassification errors, with higher weights assigned to misclassified instances, so that subsequent weak learners focus more on these difficult-to-classify instances.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8712ab-e106-4774-af4b-7db3b8c6ad53",
   "metadata": {},
   "source": [
    "## Q9. How does the AdaBoost algorithm update the weights of misclassified samples?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f17480e-d151-4205-bf29-fd5a13d030b1",
   "metadata": {},
   "source": [
    "# Weight Update of Misclassified Samples in AdaBoost Algorithm\n",
    "\n",
    "In the AdaBoost algorithm, the weights of misclassified samples are updated using a specific formula. Here's how the AdaBoost algorithm updates the weights of misclassified samples:\n",
    "\n",
    "1. **Initialization**: Initially, all samples have equal weights.\n",
    "\n",
    "2. **Training a Weak Learner**: After training a weak learner on the weighted dataset, the algorithm evaluates its performance and calculates the weighted error rate.\n",
    "\n",
    "3. **Weight Update**:\n",
    "   - For each sample \\( i \\):\n",
    "     - If the weak learner misclassifies sample \\( i \\), its weight is increased.\n",
    "     - If the weak learner correctly classifies sample \\( i \\), its weight remains unchanged.\n",
    "\n",
    "4. **Normalization**: After updating the weights, they are normalized to ensure they sum up to one. This normalization step ensures that the weights remain valid probabilities.\n",
    "\n",
    "The specific formula used to update the weights of misclassified samples is as follows:\n",
    "\n",
    "\\[ w_i^{(t+1)} = w_i^{(t)} \\times \\exp(\\alpha_t) \\]\n",
    "\n",
    "Where:\n",
    "- \\( w_i^{(t)} \\) is the weight of sample \\( i \\) at iteration \\( t \\).\n",
    "- \\( \\alpha_t \\) is the weight of the weak learner at iteration \\( t \\).\n",
    "\n",
    "This formula effectively increases the weights of misclassified samples, making them more influential in subsequent iterations of the algorithm. As a result, subsequent weak learners focus more on these hard-to-classify samples, gradually improving the overall performance of the AdaBoost model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46374253-9926-4a36-aea7-9681a8ff581f",
   "metadata": {},
   "source": [
    "## Q10. What is the effect of increasing the number of estimators in AdaBoost algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c4285f-30c5-4fd2-81ba-6b0ec0ac23a1",
   "metadata": {},
   "source": [
    "# Effect of Increasing the Number of Estimators in AdaBoost Algorithm\n",
    "\n",
    "Increasing the number of estimators in the AdaBoost algorithm can have several effects on the model's performance and behavior:\n",
    "\n",
    "1. **Improved Training Accuracy**: Generally, increasing the number of estimators allows the AdaBoost algorithm to fit the training data more closely. This can lead to improved training accuracy as the model becomes more expressive and can capture more complex patterns in the data.\n",
    "\n",
    "2. **Reduced Bias**: With more estimators, the AdaBoost model has a higher capacity to represent the underlying relationship between features and labels in the training data. This can reduce bias in the model, allowing it to better capture the true decision boundary of the data.\n",
    "\n",
    "3. **Increased Model Complexity**: As the number of estimators increases, the AdaBoost model becomes more complex. This complexity can lead to overfitting if the number of estimators is too high relative to the size and complexity of the training data. Regularization techniques such as limiting the maximum depth of individual estimators or using early stopping can help mitigate overfitting.\n",
    "\n",
    "4. **Slower Training Time**: Training an AdaBoost model with a larger number of estimators requires more computational resources and time. Each additional estimator increases the training time linearly, so training time can become a limiting factor when increasing the number of estimators.\n",
    "\n",
    "5. **Diminishing Returns**: There may be diminishing returns associated with increasing the number of estimators. After a certain point, adding more estimators may only marginally improve performance while significantly increasing computational cost.\n",
    "\n",
    "6. **Improved Generalization**: Despite the risk of overfitting, increasing the number of estimators in AdaBoost can lead to improved generalization performance on unseen data. This is because the ensemble model becomes more robust and better able to capture the underlying patterns in the data.\n",
    "\n",
    "Overall, increasing the number of estimators in the AdaBoost algorithm can lead to improved performance and generalization, but careful consideration should be given to the trade-offs in terms of model complexity, training time, and risk of overfitting. Cross-validation and performance evaluation on a validation set can help determine the optimal number of estimators for a given dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee8759e-866a-47e6-b2b2-fe53dab53d1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
