{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267c69ec-f770-4c66-8e3d-39883e23dd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its\n",
    "application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa6f900-c2e3-460d-925b-9c2dd72122b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Min-Max scaling is a data normalization technique used in data preprocessing to scale features to a fixed range, usually between 0 and 1. This method rescales the data by subtracting the minimum value of the feature and then dividing by the range (maximum value minus minimum value) of the feature.\n",
    "\n",
    "The formula for Min-Max scaling is:\n",
    "\n",
    "\n",
    " \n",
    "‚ÄãXscaled = (X-Xmin)/(Xmax-Xmin)\n",
    " \n",
    "\n",
    "where:\n",
    "\n",
    "X \n",
    "scaled is the scaled value of \n",
    "\n",
    "‚Äã\n",
    "  is the maximum value of the feature.\n",
    "Here's an example to illustrate Min-Max scaling:\n",
    "\n",
    "Suppose you have a dataset with a feature \"Age\" with values ranging from 20 to 60. To apply Min-Max scaling to this feature, you'd follow these steps:\n",
    "\n",
    "Find the minimum value of Age (\n",
    "ùëã\n",
    "min\n",
    "X \n",
    "min\n",
    "‚Äã\n",
    " ) = 20.\n",
    "Find the maximum value of Age (\n",
    "ùëã\n",
    "max\n",
    "X \n",
    "max\n",
    "‚Äã\n",
    " ) = 60.\n",
    "Apply the Min-Max scaling formula to each value of Age:\n",
    "For the minimum value (20):\n",
    "ùëã\n",
    "scaled\n",
    "=\n",
    "20\n",
    "‚àí\n",
    "20\n",
    "60\n",
    "‚àí\n",
    "20\n",
    "=\n",
    "0\n",
    "X \n",
    "scaled\n",
    "‚Äã\n",
    " = \n",
    "60‚àí20\n",
    "20‚àí20\n",
    "‚Äã\n",
    " =0\n",
    "For the maximum value (60):\n",
    "ùëã\n",
    "scaled\n",
    "=\n",
    "60\n",
    "‚àí\n",
    "20\n",
    "60\n",
    "‚àí\n",
    "20\n",
    "=\n",
    "1\n",
    "X \n",
    "scaled\n",
    "‚Äã\n",
    " = \n",
    "60‚àí20\n",
    "60‚àí20\n",
    "‚Äã\n",
    " =1\n",
    "Scale all other values of Age using the same formula.\n",
    "After scaling, the values of Age will be transformed to the range between 0 and 1. This normalization ensures that all features are on a similar scale, preventing features with larger magnitudes from dominating the learning algorithm and improving the performance of machine learning models that rely on distance metrics or gradient descent optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9601add4-1e0a-452d-9dd7-419be4e53d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling?\n",
    "Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3bb1d1-7649-4e73-b655-075bb575edfb",
   "metadata": {},
   "source": [
    "The Unit Vector technique, also known as Unit Length or Vector Normalization, is a feature scaling method that scales each feature vector to have a length of 1, while preserving the direction of the vector. It differs from Min-Max scaling in that it doesn't necessarily scale the features to a specific range like 0 to 1, but rather ensures that each feature vector has a magnitude of 1.\n",
    "\n",
    "The formula for Unit Vector scaling is:\n",
    "\n",
    "\\[ X_{\\text{unit}} = \\frac{X}{||X||} \\]\n",
    "\n",
    "where:\n",
    "- \\( X_{\\text{unit}} \\) is the unit-scaled value of \\( X \\),\n",
    "- \\( X \\) is the original value of the feature vector,\n",
    "- \\( ||X|| \\) denotes the Euclidean norm (magnitude) of the feature vector \\( X \\).\n",
    "\n",
    "Here's an example to illustrate the Unit Vector technique:\n",
    "\n",
    "Suppose you have a dataset with two features, \\( X_1 \\) and \\( X_2 \\), represented as a feature vector \\( X = [x_1, x_2] \\). The original feature vector has values \\( X = [3, 4] \\).\n",
    "\n",
    "To apply Unit Vector scaling:\n",
    "\n",
    "1. Calculate the Euclidean norm (\\( ||X|| \\)) of the feature vector \\( X \\):\n",
    "   \\[ ||X|| = \\sqrt{x_1^2 + x_2^2} = \\sqrt{3^2 + 4^2} = \\sqrt{9 + 16} = \\sqrt{25} = 5 \\]\n",
    "2. Divide each component of the feature vector by its Euclidean norm:\n",
    "   \\[ X_{\\text{unit}} = \\left[\\frac{3}{5}, \\frac{4}{5}\\right] \\]\n",
    "\n",
    "After applying Unit Vector scaling, the feature vector \\( X \\) will have a length of 1, preserving its direction. In this example, the scaled feature vector \\( X_{\\text{unit}} \\) is \\( [0.6, 0.8] \\).\n",
    "\n",
    "Unit Vector scaling is particularly useful in algorithms that rely on the direction of the feature vectors, such as clustering algorithms or algorithms that compute similarities between vectors (e.g., cosine similarity). It ensures that the scale of the features doesn't affect the results, focusing solely on the direction of the vectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f6cb7e-8033-40d7-a076-10f8eeffe1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an\n",
    "example to illustrate its application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d133dc-1351-4475-9550-556e1f66edee",
   "metadata": {},
   "outputs": [],
   "source": [
    "Principal Component Analysis (PCA) is a statistical technique used for dimensionality reduction in data analysis and machine learning. \n",
    "Its primary goal is to reduce the dimensionality of a dataset while preserving most of its variance. PCA achieves this by transforming the \n",
    "original features into a new set of orthogonal (uncorrelated) features called principal components.\n",
    "\n",
    "Here's how PCA works:\n",
    "\n",
    "Centering the Data: PCA first centers the data by subtracting the mean of each feature. This step ensures that the data is centered around the origin.\n",
    "Computing the Covariance Matrix: PCA then calculates the covariance matrix of the centered data. The covariance matrix represents the relationships\n",
    "between different features in the dataset.\n",
    "Eigenvalue Decomposition: Next, PCA performs eigenvalue decomposition on the covariance matrix to find its eigenvectors and eigenvalues. \n",
    "The eigenvectors represent the directions (or principal components) of maximum variance in the data, while the corresponding eigenvalues represent\n",
    "the magnitude of variance along each eigenvector.\n",
    "Selecting Principal Components: PCA ranks the eigenvectors based on their corresponding eigenvalues in descending order. The eigenvectors with the \n",
    "highest eigenvalues capture the most variance in the data and are selected as the principal components.\n",
    "Projection: Finally, PCA projects the original data onto the subspace spanned by the selected principal components. This projection effectively \n",
    "reduces the dimensionality of the data while preserving as much variance as possible.\n",
    "PCA is widely used in various applications, including data visualization, feature extraction, and noise reduction. It helps in reducing the \n",
    "computational complexity of machine learning algorithms, improving model performance, and gaining insights into the underlying structure of the data.\n",
    "\n",
    "Here's an example to illustrate PCA's application:\n",
    "\n",
    "Suppose you have a dataset containing information about houses, including features such as the size of the house (in square feet), the number of \n",
    "bedrooms, the number of bathrooms, and the price of the house. You want to reduce the dimensionality of the dataset while preserving most of its\n",
    "variance.\n",
    "\n",
    "You can use PCA to achieve this:\n",
    "\n",
    "Center the data by subtracting the mean of each feature.\n",
    "Compute the covariance matrix of the centered data.\n",
    "Perform eigenvalue decomposition on the covariance matrix to find the principal components.\n",
    "Select a subset of principal components that capture most of the variance in the data.\n",
    "Project the original data onto the subspace spanned by the selected principal components.\n",
    "After applying PCA, you'll obtain a reduced-dimensional representation of the dataset that retains most of its variance. \n",
    "This reduced representation can be used for further analysis or as input to machine learning algorithms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb39579-c087-4ddd-9c12-99fa27a801aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature\n",
    "Extraction? Provide an example to illustrate this concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f627731-b6dc-43e1-a581-30251ee78811",
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA and feature extraction are closely related concepts in machine learning and data analysis. Feature extraction involves transforming the original\n",
    "features of a dataset into a new set of features that capture relevant information while reducing redundancy and noise. PCA can be used as a feature\n",
    "extraction technique to achieve this goal.\n",
    "\n",
    "Here's how PCA can be used for feature extraction:\n",
    "\n",
    "Dimensionality Reduction: PCA is primarily a dimensionality reduction technique that projects the original data onto a lower-dimensional subspace \n",
    "spanned by the principal components. By selecting a subset of principal components that capture most of the variance in the data, PCA effectively \n",
    "reduces the dimensionality of the dataset.\n",
    "Feature Representation: The principal components obtained from PCA can be interpreted as new features that represent combinations of the original \n",
    "features. These new features are orthogonal (uncorrelated) to each other and are ordered based on the amount of variance they capture in the data.\n",
    "Feature Selection: PCA automatically selects the most informative features (principal components) by ranking them based on their corresponding \n",
    "eigenvalues. This helps in reducing the dimensionality of the dataset while retaining most of the variance, thus improving computational efficiency\n",
    "and reducing the risk of overfitting.\n",
    "Noise Reduction: PCA can also help in removing noise and irrelevant information from the dataset by capturing only the significant patterns and \n",
    "structures in the data. This is particularly useful when dealing with high-dimensional datasets with a large number of noisy or redundant features.\n",
    "Here's an example to illustrate how PCA can be used for feature extraction:\n",
    "\n",
    "Suppose you have a dataset containing images of handwritten digits, each represented as a feature vector of pixel values. Each image has thousands\n",
    "of pixels, making the dataset high-dimensional and computationally expensive to process.\n",
    "\n",
    "You can use PCA for feature extraction in the following steps:\n",
    "\n",
    "Data Preprocessing: Preprocess the image data by flattening each image into a one-dimensional feature vector and centering the data by subtracting\n",
    "the mean.\n",
    "PCA: Apply PCA to the preprocessed data to obtain the principal components. Each principal component represents a linear combination of the original\n",
    "pixel values and captures different patterns and structures present in the images.\n",
    "Dimensionality Reduction: Select a subset of principal components that capture most of the variance in the data. This effectively reduces the\n",
    "dimensionality of the feature space while retaining the essential information needed for classification or other tasks.\n",
    "Feature Representation: The selected principal components serve as new features that represent the images in a lower-dimensional space. These \n",
    "features can be used as input to machine learning algorithms for tasks such as image classification or clustering.\n",
    "By using PCA for feature extraction, you can reduce the dimensionality of the image dataset while preserving important patterns and structures, \n",
    "leading to more efficient and accurate machine learning models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a8b8dc-00bc-44d4-871e-3d698226a884",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset\n",
    "contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to\n",
    "preprocess the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4f9366-d86f-4ced-b96a-0f6276eecf40",
   "metadata": {},
   "source": [
    "To preprocess the data for building a recommendation system for a food delivery service using Min-Max scaling, follow these steps:\n",
    "\n",
    "1. **Understand the Data**: Begin by understanding the dataset and the features it contains. In this case, the dataset includes features such as price, rating, and delivery time.\n",
    "\n",
    "2. **Min-Max Scaling**: Min-Max scaling is used to scale each feature to a fixed range, usually between 0 and 1. This ensures that all features have a similar scale and prevents features with larger magnitudes from dominating the recommendation process.\n",
    "\n",
    "3. **Calculate Min and Max Values**: For each feature (price, rating, delivery time), calculate the minimum and maximum values in the dataset.\n",
    "\n",
    "4. **Apply Min-Max Scaling**: For each feature \\(X\\), apply the Min-Max scaling formula:\n",
    "\n",
    "\\[ X_{\\text{scaled}} = \\frac{X - X_{\\text{min}}}{X_{\\text{max}} - X_{\\text{min}}} \\]\n",
    "\n",
    "where:\n",
    "- \\( X_{\\text{scaled}} \\) is the scaled value of \\( X \\),\n",
    "- \\( X_{\\text{min}} \\) is the minimum value of the feature,\n",
    "- \\( X_{\\text{max}} \\) is the maximum value of the feature.\n",
    "\n",
    "5. **Normalize the Data**: Replace the original values of each feature with their scaled values obtained from the Min-Max scaling process.\n",
    "\n",
    "6. **Use the Preprocessed Data**: The preprocessed data, with features scaled using Min-Max scaling, can now be used as input to build the recommendation system. Algorithms such as collaborative filtering or content-based filtering can be applied to recommend food items based on user preferences, taking into account features like price, rating, and delivery time.\n",
    "\n",
    "By preprocessing the data using Min-Max scaling, you ensure that all features have a consistent scale, which can improve the performance and accuracy of the recommendation system. It also helps in handling features with different ranges and units, making the data more suitable for modeling and analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23286e8a-a8a9-4299-a8cc-4ef3be319b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. You are working on a project to build a model to predict stock prices. The dataset contains many\n",
    "features, such as company financial data and market trends. Explain how you would use PCA to reduce the\n",
    "dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88396deb-1212-4b00-8b86-aa5d66b12609",
   "metadata": {},
   "outputs": [],
   "source": [
    "To use PCA for reducing the dimensionality of the dataset for predicting stock prices, follow these steps:\n",
    "\n",
    "Data Preprocessing: Begin by preprocessing the dataset. This involves handling missing values, normalizing or standardizing the features, and \n",
    "          ensuring that the data is in a suitable format for analysis.\n",
    "Feature Selection: Identify the features that are relevant for predicting stock prices. These could include company financial data (e.g., revenue, \n",
    "    earnings, debt-to-equity ratio) and market trends (e.g., stock market indices, interest rates, economic indicators).\n",
    "Centering the Data: Center the selected features by subtracting the mean of each feature. This step ensures that the data is centered around the \n",
    "            origin, which is a requirement for PCA.\n",
    "Standardization: Standardize the centered data by dividing each feature by its standard deviation. This step ensures that all features have the same\n",
    "                scale, which is necessary for PCA to work effectively.\n",
    "PCA: Apply PCA to the standardized data to reduce its dimensionality. PCA will transform the original features into a new set of orthogonal features\n",
    "    called principal components. These principal components capture most of the variance in the data while reducing its dimensionality.\n",
    "Selecting the Number of Components: Decide on the number of principal components to retain based on the amount of variance explained by each component. \n",
    "        Typically, you can choose a number of components that explain a significant portion (e.g., 95%) of the total variance in the data.\n",
    "Dimensionality Reduction: Project the original data onto the subspace spanned by the selected principal components. This effectively reduces the \n",
    "                dimensionality of the dataset while retaining most of its information.\n",
    "Model Building: Finally, use the reduced-dimensional dataset as input to build a predictive model for stock prices. You can use various machine learning\n",
    "algorithms such as linear regression, support vector machines, or neural networks to train the model and make predictions.\n",
    "Using PCA for dimensionality reduction in the context of predicting stock prices helps in dealing with the curse of dimensionality, reduces \n",
    "        computational complexity, and may improve the performance of the predictive model by focusing on the most important features.\n",
    "\n",
    "Keep in mind that while PCA can be effective for dimensionality reduction, it's essential to interpret the results carefully and ensure that the \n",
    "reduced-dimensional dataset captures the essential information needed for accurate predictions. Additionally, consider experimenting with different \n",
    "                                            feature sets and model architectures to find the best approach for predicting stock prices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f9785f-cf35-4237-bbc3-1c7944466909",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the\n",
    "values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cede24d-e99f-4d0e-93f2-9b50315c5d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data: [1, 5, 10, 15, 20]\n",
      "Min-Max scaled data: [-1.0, -0.5789473684210527, -0.052631578947368474, 0.4736842105263157, 1.0]\n"
     ]
    }
   ],
   "source": [
    "# Given dataset\n",
    "data = [1, 5, 10, 15, 20]\n",
    "\n",
    "# Find the minimum and maximum values\n",
    "min_val = min(data)\n",
    "max_val = max(data)\n",
    "\n",
    "# Define the range for scaling\n",
    "scaled_min = -1\n",
    "scaled_max = 1\n",
    "\n",
    "# Perform Min-Max scaling\n",
    "scaled_data = [(x - min_val) / (max_val - min_val) * (scaled_max - scaled_min) + scaled_min for x in data]\n",
    "\n",
    "print(\"Original data:\", data)\n",
    "print(\"Min-Max scaled data:\", scaled_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca531d9-2b05-41a2-873c-84a2ccb4194a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
